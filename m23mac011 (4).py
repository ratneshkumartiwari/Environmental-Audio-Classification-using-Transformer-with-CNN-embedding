# -*- coding: utf-8 -*-
"""M23MAC011.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16mHnGGuIgD4eN7_BLhF627tuIG39jLk4
"""

!pip install pytorch_lightning
!pip install wandb

# Importing Libraries
print('Importing Libraries... ',end='')
import os
from pathlib import Path
import pandas as pd
import torchaudio
import zipfile
from torchaudio.transforms import Resample
import IPython.display as ipd
from matplotlib import pyplot as plt
from tqdm import tqdm
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import ConcatDataset, DataLoader
import math
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.preprocessing import label_binarize
from itertools import cycle
import wandb

from google.colab import drive
drive.mount('/content/drive')

# Loading dataset
path = Path('/content/drive/MyDrive/Archive')
df = pd.read_csv('/content/drive/MyDrive/Archive/meta/esc50.csv')

class CustomDataset(Dataset):
    def __init__(self, dataset, **kwargs):
        # Initialize CustomDataset object with relevant parameters
        # dataset: "train", "val", or "test"
        # kwargs: Additional parameters like data directory, dataframe, folds, etc.

        # Extract parameters from kwargs
        self.data_directory = kwargs["data_directory"]
        self.data_frame = kwargs["data_frame"]
        self.validation_fold = kwargs["validation_fold"]
        self.testing_fold = kwargs["testing_fold"]
        self.esc_10_flag = kwargs["esc_10_flag"]
        self.file_column = kwargs["file_column"]
        self.label_column = kwargs["label_column"]
        self.sampling_rate = kwargs["sampling_rate"]
        self.new_sampling_rate = kwargs["new_sampling_rate"]
        self.sample_length_seconds = kwargs["sample_length_seconds"]

        # Filter dataframe based on esc_10_flag and data_type
        if self.esc_10_flag:
            self.data_frame = self.data_frame.loc[self.data_frame['esc10'] == True]

        if dataset == "train":
            self.data_frame = self.data_frame.loc[
                (self.data_frame['fold'] != self.validation_fold) & (self.data_frame['fold'] != self.testing_fold)]
        elif dataset == "val":
            self.data_frame = self.data_frame.loc[self.data_frame['fold'] == self.validation_fold]
        elif dataset == "test":
            self.data_frame = self.data_frame.loc[self.data_frame['fold'] == self.testing_fold]

        # Get unique categories from the filtered dataframe
        self.categories = sorted(self.data_frame[self.label_column].unique())

        # Initialize lists to hold file names, labels, and folder numbers
        self.file_names = []
        self.labels = []

        # Initialize dictionaries for category-to-index and index-to-category mapping
        self.category_to_index = {}
        self.index_to_category = {}

        for i, category in enumerate(self.categories):
            self.category_to_index[category] = i
            self.index_to_category[i] = category

        # Populate file names and labels lists by iterating through the dataframe
        for ind in tqdm(range(len(self.data_frame))):
            row = self.data_frame.iloc[ind]
            file_path = self.data_directory / "audio" / row[self.file_column]
            self.file_names.append(file_path)
            self.labels.append(self.category_to_index[row[self.label_column]])

        self.resampler = torchaudio.transforms.Resample(self.sampling_rate, self.new_sampling_rate)

        # Window size for rolling window sample splits (unfold method)
        if self.sample_length_seconds == 2:
            self.window_size = self.new_sampling_rate * 2
            self.step_size = int(self.new_sampling_rate * 0.75)
        else:
            self.window_size = self.new_sampling_rate
            self.step_size = int(self.new_sampling_rate * 0.5)

    def __getitem__(self, index):
        # Split audio files with overlap, pass as stacked tensors tensor with a single label
        path = self.file_names[index]
        audio_file = torchaudio.load(path, format=None, normalize=True)
        audio_tensor = self.resampler(audio_file[0])
        splits = audio_tensor.unfold(1, self.window_size, self.step_size)
        samples = splits.permute(1, 0, 2)
        return samples, self.labels[index]

    def __len__(self):
        return len(self.file_names)

class CustomDataModule(pl.LightningDataModule):
    def __init__(self, **kwargs):
        # Initialize the CustomDataModule with batch size, number of workers, and other parameters
        super().__init__()
        self.batch_size = kwargs["batch_size"]
        self.num_workers = kwargs["num_workers"]
        self.data_module_kwargs = kwargs

    def setup(self, stage=None):
        # Define datasets for training, validation, and testing during Lightning setup

        # If in 'fit' or None stage, create training and validation datasets
        if stage == 'fit' or stage is None:
            self.training_dataset = CustomDataset(dataset="train", **self.data_module_kwargs)
            self.validation_dataset = CustomDataset(dataset="val", **self.data_module_kwargs)

        # If in 'test' or None stage, create testing dataset
        if stage == 'test' or stage is None:
            self.testing_dataset = CustomDataset(dataset="test", **self.data_module_kwargs)

    def train_dataloader(self):
        # Return DataLoader for training dataset
        return DataLoader(self.training_dataset,
                          batch_size=self.batch_size,
                          shuffle=True,
                          collate_fn=self.collate_function,
                          num_workers=self.num_workers)

    def val_dataloader(self):
        # Return DataLoader for validation dataset
        return DataLoader(self.validation_dataset,
                          batch_size=self.batch_size,
                          shuffle=False,
                          collate_fn=self.collate_function,
                          num_workers=self.num_workers)

    def test_dataloader(self):
        # Return DataLoader for testing dataset
        return DataLoader(self.testing_dataset,
                          batch_size=self.batch_size,
                          shuffle=False,
                          collate_fn=self.collate_function,
                          num_workers=self.num_workers)

    def collate_function(self, data):
        # """
        # Collate function to process a batch of examples and labels.

        # Args:
        #     data: a tuple of 2 tuples with (example, label) where
        #         example are the split 1 second sub-frame audio tensors per file
        #         label = the label

        # Returns:
        #     A list containing examples (concatenated tensors) and labels (flattened tensor).
        # """
        examples, labels = zip(*data)
        examples = torch.cat(examples)
        labels = torch.flatten(torch.tensor(labels))

        return [examples, labels]

# Data Setup
def split(valid_samp):
    test_samp = 1 #""" Do not change this!! """
    batch_size = 40 # Free to change
    num_workers = 2 # Free to change
    custom_data_module = CustomDataModule(batch_size=batch_size,
                                          num_workers=num_workers,
                                          data_directory=path,
                                          data_frame=df,
                                          validation_fold=valid_samp,
                                          testing_fold=test_samp,  # set to 0 for no test set
                                          esc_10_flag=True,
                                          file_column='filename',
                                          label_column='category',
                                          sampling_rate=44100,
                                          new_sampling_rate=16000,  # new sample rate for input
                                          sample_length_seconds=1  # new length of input in seconds
                                          )


    return custom_data_module

class ConvNet(nn.Module):
    def __init__(self, num_classes):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=9, out_channels=30, kernel_size=16, stride=2)
        self.conv2 = nn.Conv1d(in_channels=30, out_channels=70, kernel_size=8, stride=2)
        self.conv3 = nn.Conv1d(in_channels=70, out_channels=20, kernel_size= 10, stride=2)
        self.fc1 = nn.Linear(4940, 64)
        self.fc2 = nn.Linear(64, num_classes)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)
        self.softmax = nn.Softmax(dim=1)
        self.bn1 = nn.BatchNorm1d(30)
        self.bn2 = nn.BatchNorm1d(70)
        self.bn3 = nn.BatchNorm1d(20)
        self.dropout = nn.Dropout(0.25)

    def forward(self, x):
        x = self.bn1(self.relu(self.conv1(x)))
        x = self.pool(x)

        x = self.bn2(self.relu(self.conv2(x)))
        x = self.dropout(x)

        x = self.pool(x)

        x = self.bn3(self.relu(self.conv3(x)))
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

class Transformer(nn.Module):
    def __init__(self, heads, num_layers, num_classes):
        super(Transformer, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=9, out_channels=30, kernel_size=16, stride=2)
        self.conv2 = nn.Conv1d(in_channels=30, out_channels=70, kernel_size=8, stride=2)
        self.conv3 = nn.Conv1d(in_channels=70, out_channels=20, kernel_size= 10, stride=2)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)
        self.bn1 = nn.BatchNorm1d(30)
        self.bn2 = nn.BatchNorm1d(70)
        self.bn3 = nn.BatchNorm1d(20)
        self.dropout = nn.Dropout(0.25)
        self.transformer = TransformerEncoder(heads, num_layers, num_classes, 20, 247)

    def forward(self, x):
        x = self.bn1(self.relu(self.conv1(x)))
        x = self.pool(x)
        x = self.bn2(self.relu(self.conv2(x)))
        x = self.pool(x)
        x = self.dropout(x)
        x = self.bn3(self.relu(self.conv3(x)))
        x = self.pool(x)
        x = self.transformer(x, None)
        return x

class MultiHeadSelfAttention(torch.nn.Module):
    def __init__(self, channels, seq, heads):
        super(MultiHeadSelfAttention, self).__init__()
        self.query_projection = torch.nn.Linear(seq, seq//heads)
        self.key_projection = torch.nn.Linear(seq, seq//heads)
        self.value_projection = torch.nn.Linear(seq, seq//heads)

    def forward(self, x):
        batch_size, channels, seq = x.size()

        queries = self.query_projection(x)
        keys = self.key_projection(x)
        values = self.value_projection(x)

        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (channels ** 0.5)

        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_output = torch.matmul(attention_weights, values)

        return attention_output


class TransformerBlock(nn.Module):
    def __init__(self, channels, seq, heads):
        super(TransformerBlock, self).__init__()
        self.dropout = nn.Dropout(0.18)
        self.heads = heads
        self.seq= seq

        self.attention = MultiHeadSelfAttention(channels, seq, heads)
        self.norm1 = nn.LayerNorm(seq)
        self.norm2 = nn.LayerNorm(seq)
        self.mlp = nn.Sequential(
            nn.Linear(channels, 4 * channels),
            nn.ReLU(),
            nn.Linear(4 * channels, channels),
        )

    def forward(self, value, key, query, mask):
        attention = []
        for i in range(self.heads):
            attention.append(self.attention(key))
        attention = torch.cat(attention, dim=2)
        x = self.norm1(attention + query)
        y = x.transpose(-2,-1)
        mlp_output = self.dropout(self.mlp(y))
        mlp_output = mlp_output.transpose(-2,-1)
        return self.norm2(mlp_output + x)


class TransformerEncoder(nn.Module):
    def __init__(self,  heads, num_layers, num_classes, channels, seq):
        super(TransformerEncoder, self).__init__()
        self.dropout = nn.Dropout(0.1)
        self.heads = heads

        self.num_layers = num_layers
        self.num_classes = num_classes
        self.layers = nn.ModuleList(
            [
                TransformerBlock(channels, seq + 1, heads)
                for _ in range(num_layers)
            ]
        )
        self.fc_out = nn.Sequential(
                          nn.Linear(channels, 100),
                          nn.ReLU(),
                          nn.Linear(100, num_classes),
                      )

    def positional_encoding(self, seq_length, embedding_dim):
        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embedding_dim, 2, dtype=torch.float) * (-math.log(10000.0) / embedding_dim))
        pos_encodings = torch.zeros(seq_length, embedding_dim)
        pos_encodings[:, 0::2] = torch.sin(position * div_term)
        pos_encodings[:, 1::2] = torch.cos(position * div_term)
        return pos_encodings

    def forward(self, x, mask):
        N, channel, seq_length = x.shape
        cls_token = nn.Parameter(torch.randn(N , channel, 1)) # including cls token for classification
        x = torch.cat((cls_token, x), dim=2)
        y = self.positional_encoding(seq_length + 1, channel)
        for i in range(N):
            x[i] = x[i] + y.T
        for layer in self.layers:
            x = layer(x, x, x, mask)
        x = x[:,:, :1]
        x = x.squeeze()
        x = self.dropout(self.fc_out(x))
        return x

# Defining the training phase
def train_network(learning_rate, num_epochs, model, custom_data_module):

    # Initialize WandB
    wandb.init(project='Deep_Learning_2', entity='m23mac011')

    cross_entropy = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    epochs = num_epochs
    # model = model.cuda()
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        correct_train = 0
        total_train = 0

        # Training
        for batch in custom_data_module.train_dataloader():
            data , target = batch
            # data = data.cuda()
            # target = target.cuda()

            data = data.reshape(40,9,16000)
            optimizer.zero_grad()
            output = model(data)
            loss = cross_entropy(output, target)
            loss.backward()
            optimizer.step()
            output = F.softmax(output, dim = 1)
            output = torch.argmax(output, dim=1)
            train_loss += loss.item()
            total_train += target.size(0)
            correct_train += (output == target).sum().item()


        #Validation
        x = custom_data_module.val_dataloader()
        val_loss, val_accuracy, _ , __ , ___ =   model_test(model, x)

        # Calculate metrics
        train_loss /= 6
        train_accuracy = correct_train / total_train

        # Log metrics to WandB
        wandb.log({"epoch": epoch, "train_loss": train_loss, "val_loss": val_loss,
                   "train_accuracy": train_accuracy, "val_accuracy": val_accuracy})

        print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
              f"Train Acc: {train_accuracy*100:.2f}%, Val Acc: {val_accuracy*100:.2f}%")
    # Finish logging
    wandb.finish()
    return model

#Defining the testing phase
def model_test(model, x):
    cross_entropy = nn.CrossEntropyLoss()
    model.eval()
    test_loss = 0.0
    correct_test = 0
    total_test = 0
    targett = []
    predicted = []
    possibility = []
    # model = model.cuda()

    with torch.no_grad():
        for batch in x:
            data , target = batch
            # data = data.cuda()
            # target = target.cuda()
            data = data.reshape(40,9,16000)
            output = model(data)
            loss = cross_entropy(output, target)
            test_loss += loss.item()
            output = F.softmax(output, dim = 1)
            possibility.append(output.tolist())
            output = torch.argmax(output, dim=1)
            total_test += target.size(0)
            correct_test += (output == target).sum().item()
            targett.append(target)
            predicted.append(output)
    test_loss = test_loss/2
    test_accuracy = correct_test/total_test
    targett = [item for sublist in targett for item in sublist]
    predicted = [item for sublist in predicted for item in sublist]

    return test_loss , test_accuracy , targett, predicted, possibility

def auc_roc_curv(possibility, target):
    possibility = [item for sublist in possibility for item in sublist]
    possibility = np.array(possibility)
    target = np.array(target)
    # Binarize the true labels
    y_true_binarized = label_binarize(target, classes=range(num_classes))

    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], possibility[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_true_binarized.ravel(), possibility.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

    # Plot ROC curve
    plt.figure()
    lw = 2
    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
    for i, color in zip(range(num_classes), colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[i])

    plt.plot(fpr["micro"], tpr["micro"], color='deeppink', lw=lw, linestyle=':', label='Micro-average ROC curve (area = %0.2f)' % roc_auc["micro"])

    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

def plot_confusion_matrix(cm, classes):
    classes = []
    for i in range (10):
        classes.append(i)
    title='Confusion matrix'
    cmap=plt.cm.Blues

    plt.figure(figsize=(8, 6))
    sns.set(font_scale=1.2)
    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False, square=True,
                xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.title('Confusion Matrix')
    plt.show()

def plot_f1(f1_scores):
    classes = []
    for i in range (10):
        classes.append(i)
    plt.figure(figsize=(8, 6))
    plt.bar(classes, f1_scores, color='skyblue')
    plt.xlabel('Class')
    plt.ylabel('F1 Score')
    plt.title('F1 Score per Class')
    plt.ylim(0, 1)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

#Architecture 1 for normal split
data_module = split(3)  #here 3 is random number choosen from 2 to 5
data_module.setup()
num_classes = 10  # ESC10 has 10 classes (0 to 9)
model = ConvNet(num_classes=num_classes)
model = train_network(learning_rate = 0.001, num_epochs = 100, model = model , custom_data_module = data_module)

x = data_module.test_dataloader()
test_loss , test_accuracy, target, predicted , possibility = model_test(model,x)
print(f"Test Loss is ", test_loss , " Test Accuracy is " , test_accuracy*100, "%")
confusion = confusion_matrix(target, predicted)

f1 = f1_score(target, predicted, average='macro')

#auc roc curve
auc_roc_curv(possibility, target)
print("\n")
plot_confusion_matrix(confusion, 10)
print("\n")
plot_f1(f1)
print("\n")

# Count trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# Count non-trainable parameters
non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)

print("Number of trainable parametres are ", trainable_params)
print("Number of non trainable parametres are ", non_trainable_params)

#Architecture 2 with normal split

data_module = split(3)  #here 3 is random number choosen from 2 to 5
data_module.setup()
num_classes = 10  # ESC10 has 10 classes (0 to 9)
model = Transformer(heads = 1, num_layers= 2, num_classes = 10)
model = train_network(learning_rate = 0.001, num_epochs = 100, model = model, custom_data_module = data_module)
x = data_module.test_dataloader()
test_loss , test_accuracy, target, predicted , possibility = model_test(model,x)
print(f"Test Loss is ", test_loss , " Test Accuracy is " , test_accuracy*100, "%")

confusion = confusion_matrix(target, predicted)

f1 = f1_score(target, predicted, average='macro')

#auc roc curve
auc_roc_curv(possibility, target)
print("\n")
plot_confusion_matrix(confusion, 10)
print("\n")
plot_f1(f1)
print("\n")

# Count trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# Count non-trainable parameters
non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)

print("Number of trainable parametres are ", trainable_params)
print("Number of non trainable parametres are ", non_trainable_params)

#Architecture 3 with normal split
data_module = split(3)  #here 3 is random number choosen from 2 to 5
data_module.setup()
num_classes = 10  # ESC10 has 10 classes (0 to 9)
model = Transformer(heads = 2, num_layers= 2, num_classes = 10)
model = train_network(learning_rate = 0.001, num_epochs = 100, model = model, custom_data_module = data_module)
x = data_module.test_dataloader()
test_loss , test_accuracy, target, predicted , possibility = model_test(model,x)
print(f"Test Loss is ", test_loss , " Test Accuracy is " , test_accuracy*100, "%")


confusion = confusion_matrix(target, predicted)

f1 = f1_score(target, predicted, average='macro')

#auc roc curve
auc_roc_curv(possibility, target)
print("\n")
plot_confusion_matrix(confusion, 10)
print("\n")
plot_f1(f1)
print("\n")

# Count trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# Count non-trainable parameters
non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)

print("Number of trainable parametres are ", trainable_params)
print("Number of non trainable parametres are ", non_trainable_params)

#Architecture 4 with normal split
data_module = split(3)  #here 3 is random number choosen from 2 to 5
data_module.setup()
num_classes = 10  # ESC10 has 10 classes (0 to 9)
model = Transformer(heads = 4, num_layers= 2, num_classes = 10)
model = train_network(learning_rate = 0.001, num_epochs = 100, model = model, custom_data_module = data_module)
x = data_module.test_dataloader()
test_loss , test_accuracy, target, predicted , possibility = model_test(model,x)
print(f"Test Loss is ", test_loss , " Test Accuracy is " , test_accuracy*100, "%")


confusion = confusion_matrix(target, predicted)

f1 = f1_score(target, predicted, average='macro')

#auc roc curve
auc_roc_curv(possibility, target)
print("\n")
plot_confusion_matrix(confusion, 10)
print("\n")
plot_f1(f1)
print("\n")

# Count trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# Count non-trainable parameters
non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)

print("Number of trainable parametres are ", trainable_params)
print("Number of non trainable parametres are ", non_trainable_params)

#Architecture 1 with KFold:
num_epochs = 100
num_classes = 10  # ESC10 has 10 classes (0 to 9)
for i in range(2, 6):
    print(i-1, "th fold")
    data_module = split(i)  #here 3 is random number choosen from 2 to 5
    data_module.setup()
    num_classes = 10  # ESC10 has 10 classes (0 to 9)
    model = ConvNet(num_classes = 10)
    model = train_network(learning_rate = 0.001, num_epochs = 100, model = model, custom_data_module = data_module)
    x = data_module.test_dataloader()
    test_loss , test_accuracy, target, predicted , possibility = model_test(model,x)
    print(f"Test Loss is ", test_loss , " Test Accuracy is " , test_accuracy*100, "%")


    confusion = confusion_matrix(target, predicted)

    f1 = f1_score(target, predicted, average='macro')

    #auc roc curve
    auc_roc_curv(possibility, target)
    print("\n")
    plot_confusion_matrix(confusion, 10)
    print("\n")
    plot_f1(f1)
    print("\n")

    # Count trainable parameters
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    # Count non-trainable parameters
    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)

    print("Number of trainable parametres are ", trainable_params)
    print("Number of non trainable parametres are ", non_trainable_params)

#Architecture 1 with KFold:
num_epochs = 100
num_classes = 10  # ESC10 has 10 classes (0 to 9)
for i in range(2, 6):
    print(i-1, "th fold")
    data_module = split(i)  #here 3 is random number choosen from 2 to 5
    data_module.setup()
    num_classes = 10  # ESC10 has 10 classes (0 to 9)
    model = Transformer(heads = 4, num_layers= 2, num_classes = 10)
    model = train_network(learning_rate = 0.001, num_epochs = 100, model = model, custom_data_module = data_module)
    x = data_module.test_dataloader()
    test_loss , test_accuracy, target, predicted , possibility = model_test(model,x)
    print(f"Test Loss is ", test_loss , " Test Accuracy is " , test_accuracy*100, "%")


    confusion = confusion_matrix(target, predicted)

    f1 = f1_score(target, predicted, average='macro')

    #auc roc curve
    auc_roc_curv(possibility, target)
    print("\n")
    plot_confusion_matrix(confusion, 10)
    print("\n")
    plot_f1(f1)
    print("\n")

    # Count trainable parameters
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    # Count non-trainable parameters
    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)

    print("Number of trainable parametres are ", trainable_params)
    print("Number of non trainable parametres are ", non_trainable_params)

#Architecture 2 with KFold:
num_epochs = 100
num_classes = 10  # ESC10 has 10 classes (0 to 9)
for i in range(2, 6):
    print(i-1, "th fold")
    data_module = split(i)  #here 3 is random number choosen from 2 to 5
    data_module.setup()
    num_classes = 10  # ESC10 has 10 classes (0 to 9)
    model = Transformer(heads = 2, num_layers= 2, num_classes = 10)
    model = train_network(learning_rate = 0.001, num_epochs = 100, model = model, custom_data_module = data_module)
    x = data_module.test_dataloader()
    test_loss , test_accuracy, target, predicted , possibility = model_test(model,x)
    print(f"Test Loss is ", test_loss , " Test Accuracy is " , test_accuracy*100, "%")

    confusion = confusion_matrix(target, predicted)

    f1 = f1_score(target, predicted, average='macro')

    #auc roc curve
    auc_roc_curv(possibility, target)
    print("\n")
    plot_confusion_matrix(confusion, 10)
    print("\n")
    plot_f1(f1)
    print("\n")

    # Count trainable parameters
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    # Count non-trainable parameters
    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)

    print("Number of trainable parametres are ", trainable_params)
    print("Number of non trainable parametres are ", non_trainable_params)

#Architecture 2 with KFold:
num_epochs = 100
num_classes = 10  # ESC10 has 10 classes (0 to 9)
for i in range(2, 6):
    print(i-1, "th fold")
    data_module = split(i)  #here 3 is random number choosen from 2 to 5
    data_module.setup()
    num_classes = 10  # ESC10 has 10 classes (0 to 9)
    model = Transformer(heads = 1, num_layers= 2, num_classes = 10)
    model = train_network(learning_rate = 0.001, num_epochs = 100, model = model, custom_data_module = data_module)
    x = data_module.test_dataloader()
    test_loss , test_accuracy, target, predicted , possibility = model_test(model,x)
    print(f"Test Loss is ", test_loss , " Test Accuracy is " , test_accuracy*100, "%")


    confusion = confusion_matrix(target, predicted)

    f1 = f1_score(target, predicted, average='macro')

    #auc roc curve
    auc_roc_curv(possibility, target)
    print("\n")
    plot_confusion_matrix(confusion, 10)
    print("\n")
    plot_f1(f1)
    print("\n")

    # Count trainable parameters
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    # Count non-trainable parameters
    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)

    print("Number of trainable parametres are ", trainable_params)
    print("Number of non trainable parametres are ", non_trainable_params)

#Hyperparameter Tuning

lr = [0.0001,  0.0005, 0.001, 0.005]

for i in lr:
    data_module = split(3)  #here 3 is random number choosen from 2 to 5
    data_module.setup()
    print("for learning rate = ", i)
    num_classes = 10  # ESC10 has 10 classes (0 to 9)
    model = ConvNet(num_classes=num_classes)
    model = train_network(learning_rate = i, num_epochs = 15, model = model , custom_data_module = data_module)

#Hyperparameter Tuning

lr = [0.0001,  0.0005, 0.001, 0.005]

for i in lr:
    data_module = split(3)  #here 3 is random number choosen from 2 to 5
    data_module.setup()
    print("for learning rate = ", i)
    num_classes = 10  # ESC10 has 10 classes (0 to 9)
    model = Transformer(heads = 1, num_layers= 2, num_classes = 10)
    model = train_network(learning_rate = i, num_epochs = 15, model = model , custom_data_module = data_module)

#Hyperparameter Tuning

lr = [0.0001,  0.0005, 0.001, 0.005]

for i in lr:
    data_module = split(3)  #here 3 is random number choosen from 2 to 5
    data_module.setup()
    print("for learning rate = ", i)
    num_classes = 10  # ESC10 has 10 classes (0 to 9)
    model = Transformer(heads = 2, num_layers= 2, num_classes = 10)
    model = train_network(learning_rate = i, num_epochs = 15, model = model , custom_data_module = data_module)

#Hyperparameter Tuning

lr = [0.0001,  0.0005, 0.001, 0.005]

for i in lr:
    data_module = split(3)  #here 3 is random number choosen from 2 to 5
    data_module.setup()
    print("for learning rate = ", i)
    num_classes = 10  # ESC10 has 10 classes (0 to 9)
    model = Transformer(heads = 4, num_layers= 2, num_classes = 10)
    model = train_network(learning_rate = i, num_epochs = 15, model = model , custom_data_module = data_module)